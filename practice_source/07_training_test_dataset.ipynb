{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"07_training_test_dataset.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPabEnYIlb0CnnJmpRv8HVa"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arX4mThLF2bz","executionInfo":{"status":"ok","timestamp":1622273544748,"user_tz":-540,"elapsed":2782,"user":{"displayName":"최지수","photoUrl":"","userId":"02873560556130811520"}},"outputId":"853e523f-8177-436e-b6af-9bba70202dc5"},"source":["import tensorflow as tf\n","import numpy as np\n","\n","x_data = [[1, 2, 1],\n","          [1, 3, 2],\n","          [1, 3, 4],\n","          [1, 5, 5],\n","          [1, 7, 5],\n","          [1, 2, 5],\n","          [1, 6, 6],\n","          [1, 7, 7]]\n","y_data = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [0, 1, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]]\n","\n","# Test 용 Dataset\n","x_test = [[2, 1, 1],\n","          [3, 1, 2],\n","          [3, 3, 4]]\n","y_test = [[0, 0, 1],\n","          [0, 0, 1],\n","          [0, 0, 1]]\n","\n","x_data = tf.cast(x_data, dtype=np.float32)\n","y_data = tf.cast(y_data, dtype=np.float32)\n","\n","# batch : 한번에 학습시킬 Size\n","dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n","\n","W = tf.Variable(tf.random.normal([3, 3]), name='weight')\n","b = tf.Variable(tf.random.normal([3]), name='bias')\n","\n","def softmax_fn(features):\n","    hypothesis = tf.nn.softmax(tf.matmul(features, W) + b)\n","    return hypothesis\n","\n","def cost_fn(features, labels):\n","    # hypothesis = softmax_fn(features)\n","    # cost = tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(hypothesis), 1))\n","    logits = tf.matmul(features, W) + b\n","    cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels)\n","    \n","    cost = tf.reduce_mean(cost_i)\n","\n","    return cost\n","\n","def grad(features, labels):\n","    with tf.GradientTape() as tape:\n","        cost = cost_fn(features, labels)\n","        grads = tape.gradient(cost, [W, b])\n","\n","    return grads\n","\n","def prediction(features, labels):\n","    pred = tf.argmax(softmax_fn(features), 1)\n","    correct_pred = tf.equal(pred, tf.argmax(labels, 1))\n","    accuracy = tf.reduce_mean(tf.cast(correct_pred, dtype=tf.float32))\n","\n","    return accuracy\n","\n","# lr == 10: 최소값으로 가지않고 와리가리함\n","# lr == 1e-10: 이동 값이 너무 작음\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n","\n","for step in range(201):\n","    for features, labels in iter(dataset):\n","        grads = grad(features, labels)\n","        optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n","        cost = cost_fn(features, labels).numpy()\n","        acc = prediction(features, labels).numpy()\n","        print(\"iter : {}, cost : {}, accuracy : {:.4f}\".format(step, cost, acc))\n","\n"],"execution_count":20,"outputs":[{"output_type":"stream","text":["iter : 0, cost : 6.793728828430176, accuracy : 0.3750\n","iter : 1, cost : 3.7471084594726562, accuracy : 0.0000\n","iter : 2, cost : 2.736165761947632, accuracy : 0.2500\n","iter : 3, cost : 1.792798399925232, accuracy : 0.0000\n","iter : 4, cost : 1.3895978927612305, accuracy : 0.1250\n","iter : 5, cost : 1.3315458297729492, accuracy : 0.2500\n","iter : 6, cost : 1.3132015466690063, accuracy : 0.1250\n","iter : 7, cost : 1.2959327697753906, accuracy : 0.2500\n","iter : 8, cost : 1.2793077230453491, accuracy : 0.2500\n","iter : 9, cost : 1.2632086277008057, accuracy : 0.2500\n","iter : 10, cost : 1.2475900650024414, accuracy : 0.2500\n","iter : 11, cost : 1.2324328422546387, accuracy : 0.2500\n","iter : 12, cost : 1.2177250385284424, accuracy : 0.2500\n","iter : 13, cost : 1.2034578323364258, accuracy : 0.2500\n","iter : 14, cost : 1.189622402191162, accuracy : 0.2500\n","iter : 15, cost : 1.1762102842330933, accuracy : 0.2500\n","iter : 16, cost : 1.1632124185562134, accuracy : 0.2500\n","iter : 17, cost : 1.150619387626648, accuracy : 0.2500\n","iter : 18, cost : 1.1384215354919434, accuracy : 0.2500\n","iter : 19, cost : 1.1266083717346191, accuracy : 0.2500\n","iter : 20, cost : 1.1151695251464844, accuracy : 0.2500\n","iter : 21, cost : 1.1040940284729004, accuracy : 0.2500\n","iter : 22, cost : 1.093371033668518, accuracy : 0.2500\n","iter : 23, cost : 1.0829894542694092, accuracy : 0.2500\n","iter : 24, cost : 1.0729379653930664, accuracy : 0.2500\n","iter : 25, cost : 1.0632052421569824, accuracy : 0.3750\n","iter : 26, cost : 1.0537805557250977, accuracy : 0.3750\n","iter : 27, cost : 1.0446524620056152, accuracy : 0.3750\n","iter : 28, cost : 1.0358102321624756, accuracy : 0.3750\n","iter : 29, cost : 1.0272432565689087, accuracy : 0.5000\n","iter : 30, cost : 1.0189406871795654, accuracy : 0.5000\n","iter : 31, cost : 1.010892629623413, accuracy : 0.5000\n","iter : 32, cost : 1.0030889511108398, accuracy : 0.5000\n","iter : 33, cost : 0.995519757270813, accuracy : 0.5000\n","iter : 34, cost : 0.9881759285926819, accuracy : 0.5000\n","iter : 35, cost : 0.9810482263565063, accuracy : 0.5000\n","iter : 36, cost : 0.974128007888794, accuracy : 0.5000\n","iter : 37, cost : 0.9674066305160522, accuracy : 0.5000\n","iter : 38, cost : 0.9608761668205261, accuracy : 0.5000\n","iter : 39, cost : 0.95452880859375, accuracy : 0.5000\n","iter : 40, cost : 0.9483568668365479, accuracy : 0.5000\n","iter : 41, cost : 0.942353367805481, accuracy : 0.5000\n","iter : 42, cost : 0.9365114569664001, accuracy : 0.5000\n","iter : 43, cost : 0.9308245182037354, accuracy : 0.6250\n","iter : 44, cost : 0.9252864122390747, accuracy : 0.6250\n","iter : 45, cost : 0.9198908805847168, accuracy : 0.6250\n","iter : 46, cost : 0.9146325588226318, accuracy : 0.6250\n","iter : 47, cost : 0.9095057845115662, accuracy : 0.6250\n","iter : 48, cost : 0.9045054316520691, accuracy : 0.6250\n","iter : 49, cost : 0.8996267318725586, accuracy : 0.6250\n","iter : 50, cost : 0.8948646783828735, accuracy : 0.6250\n","iter : 51, cost : 0.8902149796485901, accuracy : 0.6250\n","iter : 52, cost : 0.8856731653213501, accuracy : 0.6250\n","iter : 53, cost : 0.8812353610992432, accuracy : 0.6250\n","iter : 54, cost : 0.8768976330757141, accuracy : 0.6250\n","iter : 55, cost : 0.8726562261581421, accuracy : 0.6250\n","iter : 56, cost : 0.8685076236724854, accuracy : 0.5000\n","iter : 57, cost : 0.8644484281539917, accuracy : 0.5000\n","iter : 58, cost : 0.8604755401611328, accuracy : 0.5000\n","iter : 59, cost : 0.8565857410430908, accuracy : 0.5000\n","iter : 60, cost : 0.852776288986206, accuracy : 0.5000\n","iter : 61, cost : 0.8490442037582397, accuracy : 0.5000\n","iter : 62, cost : 0.8453870415687561, accuracy : 0.5000\n","iter : 63, cost : 0.8418021202087402, accuracy : 0.5000\n","iter : 64, cost : 0.8382871150970459, accuracy : 0.5000\n","iter : 65, cost : 0.8348395824432373, accuracy : 0.5000\n","iter : 66, cost : 0.8314574360847473, accuracy : 0.5000\n","iter : 67, cost : 0.8281384706497192, accuracy : 0.5000\n","iter : 68, cost : 0.8248806595802307, accuracy : 0.5000\n","iter : 69, cost : 0.8216822147369385, accuracy : 0.5000\n","iter : 70, cost : 0.8185411691665649, accuracy : 0.5000\n","iter : 71, cost : 0.8154557347297668, accuracy : 0.5000\n","iter : 72, cost : 0.8124241828918457, accuracy : 0.5000\n","iter : 73, cost : 0.8094450831413269, accuracy : 0.5000\n","iter : 74, cost : 0.8065166473388672, accuracy : 0.5000\n","iter : 75, cost : 0.8036375045776367, accuracy : 0.5000\n","iter : 76, cost : 0.8008062839508057, accuracy : 0.5000\n","iter : 77, cost : 0.7980213761329651, accuracy : 0.5000\n","iter : 78, cost : 0.7952816486358643, accuracy : 0.5000\n","iter : 79, cost : 0.7925857305526733, accuracy : 0.5000\n","iter : 80, cost : 0.7899326682090759, accuracy : 0.5000\n","iter : 81, cost : 0.7873209714889526, accuracy : 0.5000\n","iter : 82, cost : 0.7847496867179871, accuracy : 0.5000\n","iter : 83, cost : 0.7822176814079285, accuracy : 0.5000\n","iter : 84, cost : 0.7797238826751709, accuracy : 0.5000\n","iter : 85, cost : 0.7772672772407532, accuracy : 0.5000\n","iter : 86, cost : 0.7748469114303589, accuracy : 0.5000\n","iter : 87, cost : 0.7724618911743164, accuracy : 0.5000\n","iter : 88, cost : 0.7701115012168884, accuracy : 0.5000\n","iter : 89, cost : 0.7677944898605347, accuracy : 0.5000\n","iter : 90, cost : 0.7655103802680969, accuracy : 0.5000\n","iter : 91, cost : 0.7632579803466797, accuracy : 0.5000\n","iter : 92, cost : 0.7610368728637695, accuracy : 0.5000\n","iter : 93, cost : 0.7588461637496948, accuracy : 0.5000\n","iter : 94, cost : 0.7566852569580078, accuracy : 0.5000\n","iter : 95, cost : 0.7545531988143921, accuracy : 0.5000\n","iter : 96, cost : 0.7524493932723999, accuracy : 0.6250\n","iter : 97, cost : 0.750373363494873, accuracy : 0.6250\n","iter : 98, cost : 0.7483243346214294, accuracy : 0.6250\n","iter : 99, cost : 0.7463016510009766, accuracy : 0.6250\n","iter : 100, cost : 0.7443048357963562, accuracy : 0.6250\n","iter : 101, cost : 0.742333173751831, accuracy : 0.6250\n","iter : 102, cost : 0.7403863072395325, accuracy : 0.6250\n","iter : 103, cost : 0.7384635210037231, accuracy : 0.6250\n","iter : 104, cost : 0.7365644574165344, accuracy : 0.6250\n","iter : 105, cost : 0.734688401222229, accuracy : 0.6250\n","iter : 106, cost : 0.7328349351882935, accuracy : 0.6250\n","iter : 107, cost : 0.7310035228729248, accuracy : 0.6250\n","iter : 108, cost : 0.7291940450668335, accuracy : 0.6250\n","iter : 109, cost : 0.7274056673049927, accuracy : 0.6250\n","iter : 110, cost : 0.7256381511688232, accuracy : 0.6250\n","iter : 111, cost : 0.7238909006118774, accuracy : 0.6250\n","iter : 112, cost : 0.722163736820221, accuracy : 0.6250\n","iter : 113, cost : 0.720456063747406, accuracy : 0.6250\n","iter : 114, cost : 0.718767523765564, accuracy : 0.6250\n","iter : 115, cost : 0.7170978784561157, accuracy : 0.6250\n","iter : 116, cost : 0.7154465913772583, accuracy : 0.6250\n","iter : 117, cost : 0.7138134241104126, accuracy : 0.6250\n","iter : 118, cost : 0.7121978402137756, accuracy : 0.6250\n","iter : 119, cost : 0.7105997800827026, accuracy : 0.6250\n","iter : 120, cost : 0.7090188264846802, accuracy : 0.6250\n","iter : 121, cost : 0.7074543833732605, accuracy : 0.6250\n","iter : 122, cost : 0.7059065103530884, accuracy : 0.6250\n","iter : 123, cost : 0.7043747305870056, accuracy : 0.6250\n","iter : 124, cost : 0.7028586864471436, accuracy : 0.6250\n","iter : 125, cost : 0.7013583183288574, accuracy : 0.6250\n","iter : 126, cost : 0.6998730897903442, accuracy : 0.6250\n","iter : 127, cost : 0.6984027624130249, accuracy : 0.6250\n","iter : 128, cost : 0.6969472765922546, accuracy : 0.6250\n","iter : 129, cost : 0.69550621509552, accuracy : 0.6250\n","iter : 130, cost : 0.6940792202949524, accuracy : 0.6250\n","iter : 131, cost : 0.6926661729812622, accuracy : 0.6250\n","iter : 132, cost : 0.6912668943405151, accuracy : 0.6250\n","iter : 133, cost : 0.6898810863494873, accuracy : 0.6250\n","iter : 134, cost : 0.6885083913803101, accuracy : 0.6250\n","iter : 135, cost : 0.6871486902236938, accuracy : 0.6250\n","iter : 136, cost : 0.6858017444610596, accuracy : 0.6250\n","iter : 137, cost : 0.6844674348831177, accuracy : 0.6250\n","iter : 138, cost : 0.6831455230712891, accuracy : 0.6250\n","iter : 139, cost : 0.6818356513977051, accuracy : 0.6250\n","iter : 140, cost : 0.6805376410484314, accuracy : 0.6250\n","iter : 141, cost : 0.6792514324188232, accuracy : 0.6250\n","iter : 142, cost : 0.6779768466949463, accuracy : 0.6250\n","iter : 143, cost : 0.6767135858535767, accuracy : 0.6250\n","iter : 144, cost : 0.67546147108078, accuracy : 0.6250\n","iter : 145, cost : 0.6742203235626221, accuracy : 0.6250\n","iter : 146, cost : 0.6729901432991028, accuracy : 0.6250\n","iter : 147, cost : 0.6717703342437744, accuracy : 0.6250\n","iter : 148, cost : 0.6705611944198608, accuracy : 0.6250\n","iter : 149, cost : 0.6693623065948486, accuracy : 0.6250\n","iter : 150, cost : 0.6681735515594482, accuracy : 0.6250\n","iter : 151, cost : 0.6669947504997253, accuracy : 0.6250\n","iter : 152, cost : 0.6658258438110352, accuracy : 0.6250\n","iter : 153, cost : 0.6646666526794434, accuracy : 0.6250\n","iter : 154, cost : 0.6635168790817261, accuracy : 0.6250\n","iter : 155, cost : 0.6623765230178833, accuracy : 0.6250\n","iter : 156, cost : 0.6612454652786255, accuracy : 0.6250\n","iter : 157, cost : 0.6601235270500183, accuracy : 0.6250\n","iter : 158, cost : 0.6590104699134827, accuracy : 0.6250\n","iter : 159, cost : 0.6579062938690186, accuracy : 0.6250\n","iter : 160, cost : 0.6568108797073364, accuracy : 0.6250\n","iter : 161, cost : 0.6557238698005676, accuracy : 0.6250\n","iter : 162, cost : 0.654645562171936, accuracy : 0.6250\n","iter : 163, cost : 0.6535753011703491, accuracy : 0.6250\n","iter : 164, cost : 0.6525134444236755, accuracy : 0.6250\n","iter : 165, cost : 0.6514595746994019, accuracy : 0.6250\n","iter : 166, cost : 0.6504137516021729, accuracy : 0.6250\n","iter : 167, cost : 0.6493756771087646, accuracy : 0.6250\n","iter : 168, cost : 0.6483453512191772, accuracy : 0.6250\n","iter : 169, cost : 0.6473226547241211, accuracy : 0.6250\n","iter : 170, cost : 0.6463075876235962, accuracy : 0.6250\n","iter : 171, cost : 0.6452998518943787, accuracy : 0.6250\n","iter : 172, cost : 0.6442995071411133, accuracy : 0.6250\n","iter : 173, cost : 0.643306314945221, accuracy : 0.6250\n","iter : 174, cost : 0.6423202157020569, accuracy : 0.6250\n","iter : 175, cost : 0.6413412094116211, accuracy : 0.6250\n","iter : 176, cost : 0.6403690576553345, accuracy : 0.6250\n","iter : 177, cost : 0.6394037008285522, accuracy : 0.6250\n","iter : 178, cost : 0.6384451389312744, accuracy : 0.6250\n","iter : 179, cost : 0.6374932527542114, accuracy : 0.6250\n","iter : 180, cost : 0.636547863483429, accuracy : 0.6250\n","iter : 181, cost : 0.6356089115142822, accuracy : 0.6250\n","iter : 182, cost : 0.6346763372421265, accuracy : 0.6250\n","iter : 183, cost : 0.6337502002716064, accuracy : 0.6250\n","iter : 184, cost : 0.6328301429748535, accuracy : 0.6250\n","iter : 185, cost : 0.6319162249565125, accuracy : 0.6250\n","iter : 186, cost : 0.6310083866119385, accuracy : 0.6250\n","iter : 187, cost : 0.6301065683364868, accuracy : 0.6250\n","iter : 188, cost : 0.6292104721069336, accuracy : 0.6250\n","iter : 189, cost : 0.6283203363418579, accuracy : 0.6250\n","iter : 190, cost : 0.6274359226226807, accuracy : 0.6250\n","iter : 191, cost : 0.6265571117401123, accuracy : 0.6250\n","iter : 192, cost : 0.6256839632987976, accuracy : 0.6250\n","iter : 193, cost : 0.6248162984848022, accuracy : 0.6250\n","iter : 194, cost : 0.623954176902771, accuracy : 0.6250\n","iter : 195, cost : 0.62309730052948, accuracy : 0.6250\n","iter : 196, cost : 0.6222459077835083, accuracy : 0.6250\n","iter : 197, cost : 0.6213997006416321, accuracy : 0.6250\n","iter : 198, cost : 0.6205587387084961, accuracy : 0.6250\n","iter : 199, cost : 0.619722843170166, accuracy : 0.6250\n","iter : 200, cost : 0.6188920736312866, accuracy : 0.6250\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"s-vQi4YSF-j9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622273745128,"user_tz":-540,"elapsed":313,"user":{"displayName":"최지수","photoUrl":"","userId":"02873560556130811520"}},"outputId":"69184ab4-b015-4208-a7dd-c8706534cc91"},"source":["# 테스트 데이터로 검사\n","x_test = tf.cast(x_test, tf.float32)\n","y_test = tf.cast(y_test, tf.float32)\n","test_cost = cost_fn(x_test, y_test).numpy()\n","test_acc = prediction(x_test, y_test).numpy()\n","print(test_cost, test_acc)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["0.012063008 1.0\n"],"name":"stdout"}]}]}
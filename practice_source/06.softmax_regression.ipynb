{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"06.softmax_regression.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPzJxTjibZVxuLu/tZ49FF+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Usr8dl_YyCoi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622123858806,"user_tz":-540,"elapsed":21023,"user":{"displayName":"최지수","photoUrl":"","userId":"02873560556130811520"}},"outputId":"5b0965dd-9b56-4295-a69a-56e8d04f29f1"},"source":["import numpy as np\n","import tensorflow as tf\n","\n","x_data =  [[1, 2, 1, 1],\n","            [2, 1, 3, 2],\n","            [3, 1, 3, 4],\n","            [4, 1, 5, 5],\n","            [1, 7, 5, 5],\n","            [1, 2, 5, 6],\n","            [1, 6, 6, 6],\n","            [1, 7, 7, 7]]\n","\n","y_data =   [[0, 0, 1],\n","            [0, 0, 1],\n","            [0, 0, 1],\n","            [0, 1, 0],\n","            [0, 1, 0],\n","            [0, 1, 0],\n","            [1, 0, 0],\n","            [1, 0, 0]]\n","\n","x_data = np.asarray(x_data, dtype=np.float32)\n","y_data = np.asarray(y_data, dtype=np.float32)\n","\n","dataset = tf.data.Dataset.from_tensor_slices((x_data, y_data)).batch(len(x_data))\n","\n","W = tf.Variable(tf.random.normal([4, 3], name='weight'))\n","b = tf.Variable(tf.random.normal([3], name='bias'))\n","\n","dataset.element_spec\n","\n","def cost_fn(features, labels):\n","    hypothesis = tf.nn.softmax(tf.matmul(features, W) + b)\n","    cost = tf.reduce_mean(-tf.reduce_sum(labels * tf.math.log(hypothesis), axis=1))\n","    return cost\n","\n","def grad(features, labels):\n","    with tf.GradientTape() as tape:\n","        cost_value = cost_fn(features, labels)\n","    return tape.gradient(cost_value, [W, b])\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n","\n","EPOCHS = 2400\n","\n","for step in range(EPOCHS + 1):\n","    for features, labels in iter(dataset):\n","        grads = grad(features, labels)\n","        optimizer.apply_gradients(grads_and_vars=zip(grads, [W, b]))\n","        if step % 300 == 0:\n","            print(\"iter : {}, cost : {:.4f}\".format(step, cost_fn(features, labels)))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["iter : 0, cost : 4.3906\n","iter : 300, cost : 0.4929\n","iter : 600, cost : 0.3799\n","iter : 900, cost : 0.2760\n","iter : 1200, cost : 0.2174\n","iter : 1500, cost : 0.1905\n","iter : 1800, cost : 0.1693\n","iter : 2100, cost : 0.1522\n","iter : 2400, cost : 0.1381\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6C7yymMe6XPM","executionInfo":{"status":"ok","timestamp":1622123977589,"user_tz":-540,"elapsed":5,"user":{"displayName":"최지수","photoUrl":"","userId":"02873560556130811520"}},"outputId":"05e8171a-68cb-4067-cafb-ee98f8bfcf61"},"source":["a = x_data\n","a = tf.nn.softmax(tf.matmul(a, W) + b)\n","\n","print(\"Hypothesis : {}\".format(a))\n","\n","# argmax: 가장 큰 값의 index를 반환합니다.\n","print(tf.argmax(a, axis=1))         # 예측 값 by Hypothesis\n","print(tf.argmax(y_data, axis=1))    # 실제 값"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Hypothesis : [[6.47958529e-07 7.50564504e-04 9.99248803e-01]\n"," [1.99433096e-04 6.75592497e-02 9.32241380e-01]\n"," [5.84766902e-09 1.40294775e-01 8.59705210e-01]\n"," [8.47631796e-08 8.73523057e-01 1.26476869e-01]\n"," [2.33241811e-01 7.56763995e-01 9.99415293e-03]\n"," [1.23562574e-01 8.76400530e-01 3.68698165e-05]\n"," [7.60851204e-01 2.39121065e-01 2.77535546e-05]\n"," [9.38371420e-01 6.16282895e-02 3.26691605e-07]]\n","tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n","tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"],"name":"stdout"}]}]}